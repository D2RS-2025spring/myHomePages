# 处理回复

创建一个R脚本，用于从 GitHub Issue [#1]() 中提取回复，整理为表格，并对内容中的链接生成网页截图。

```{r}
# 安装必要的包（如果尚未安装）
if (!require("gh")) install.packages("gh")
if (!require("webshot2")) install.packages("webshot2")
if (!require("dplyr")) install.packages("dplyr")
if (!require("stringr")) install.packages("stringr")
if (!require("tibble")) install.packages("tibble")
if (!require("httr")) install.packages("httr")
if (!require("jsonlite")) install.packages("jsonlite")

# 加载包
library(gh)
library(webshot2)
library(dplyr)
library(stringr)
library(tibble)
library(httr)
library(jsonlite)

# 创建webshot文件夹（如果不存在）
if (!dir.exists("webshot")) {
  dir.create("webshot")
}
```

```{r}
# 指定仓库和Issue编号
# 请替换为你需要爬取的仓库
repo_owner <- "D2RS-2025spring"
repo_name <- "myHomePages"
issue_number <- 1

# 使用GitHub API获取issue评论
issue_comments <- gh::gh(
  "GET /repos/{owner}/{repo}/issues/{issue_number}/comments",
  owner = repo_owner,
  repo = repo_name,
  issue_number = issue_number
)

# 获取原始issue内容
issue_data <- gh::gh(
  "GET /repos/{owner}/{repo}/issues/{issue_number}",
  owner = repo_owner,
  repo = repo_name,
  issue_number = issue_number
)
```

```{r}
```


```{r}
# 创建一个空的数据框来存储评论
comments_df <- tibble(
  author = character(),
  content = character(),
  time = character(),
  links = list(),
  screenshot_paths = list()
)

# 添加原始issue作为第一条评论
comments_df <- comments_df %>%
  add_row(
    author = issue_data$user$login,
    content = issue_data$body,
    time = as.character(as.POSIXct(issue_data$created_at, format="%Y-%m-%dT%H:%M:%SZ", tz="UTC")),
    links = list(character()),
    screenshot_paths = list(character())
  )

# 处理所有评论
for (comment in issue_comments) {
  # 提取评论信息
  author <- comment$user$login
  content <- comment$body
  time <- as.character(as.POSIXct(comment$created_at, format="%Y-%m-%dT%H:%M:%SZ", tz="UTC"))
  
  # 添加到数据框
  comments_df <- comments_df %>%
    add_row(
      author = author,
      content = content,
      time = time,
      links = list(character()),
      screenshot_paths = list(character())
    )
}
```

```{r}
# 提取每条评论中的链接
for (i in 1:nrow(comments_df)) {
  # 使用正则表达式提取链接
  # 这个模式可以匹配Markdown风格的链接和直接URL
  url_pattern <- "https?://[^\\s()<>\"\\[\\]]+|\\(https?://[^\\s()<>\"\\[\\]]+\\)|\\[.*?\\]\\(https?://[^\\s()<>\"\\[\\]]+\\)"
  matches <- str_extract_all(comments_df$content[i], url_pattern)[[1]]
  
  # 清理链接（从Markdown格式中提取实际URL）
  cleaned_links <- character()
  for (match in matches) {
    # 如果是Markdown格式的链接 [text](url)
    if (str_detect(match, "\\[.*?\\]\\(.*?\\)")) {
      url <- str_extract(match, "(?<=\\().*?(?=\\))")
      cleaned_links <- c(cleaned_links, url)
    } 
    # 如果是圆括号中的链接 (url)
    else if (str_detect(match, "^\\(.*?\\)$")) {
      url <- str_sub(match, 2, -2)
      cleaned_links <- c(cleaned_links, url)
    } 
    # 直接URL
    else {
      cleaned_links <- c(cleaned_links, match)
    }
  }
  
  # 过滤掉非HTTP链接
  valid_links <- cleaned_links[str_detect(cleaned_links, "^https?://")]
  
  # 更新数据框
  comments_df$links[[i]] <- valid_links
  
  # 准备保存截图路径
  screenshot_paths <- character()
  
  # 为每个有效链接生成截图
  if (length(valid_links) > 0) {
    for (j in 1:length(valid_links)) {
      link <- valid_links[j]
      # 创建一个安全的文件名
      safe_filename <- paste0("webshot/", 
                               i, "_", j, "_", 
                               str_replace_all(gsub("https?://", "", link), "[^a-zA-Z0-9]", "_"), 
                               ".png")
      
      # 尝试截图
      tryCatch({
        webshot2::webshot(url = link, 
                          file = safe_filename, 
                          delay = 5)
        screenshot_paths <- c(screenshot_paths, safe_filename)
        cat(sprintf("成功截图: %s\n", link))
      }, error = function(e) {
        cat(sprintf("截图失败: %s, 错误: %s\n", link, e$message))
        screenshot_paths <- c(screenshot_paths, NA)
      })
    }
  }
  
  # 更新截图路径
  comments_df$screenshot_paths[[i]] <- screenshot_paths
}

# 创建最终的数据框，将链接和截图路径合并为字符串
final_df <- comments_df %>%
  mutate(
    links_str = sapply(links, function(x) paste(x, collapse = "\n")),
    screenshots_str = sapply(screenshot_paths, function(x) paste(x, collapse = "\n"))
  ) %>%
  select(author, content, time, links_str, screenshots_str) %>%
  rename(links = links_str, screenshot_paths = screenshots_str)
```

```{r}
# 保存结果到CSV
write.csv(final_df, "github_issue_comments.csv", row.names = FALSE, fileEncoding = "UTF-8")

# 输出结果
cat("处理完成! 共处理了", nrow(comments_df), "条评论\n")
cat("结果已保存到 github_issue_comments.csv\n")
print(final_df)
```

脚本说明
这个R脚本会完成以下任务：

安装并加载必要的R包
创建一个webshot文件夹用于保存网页截图
从GitHub API获取Issue #1的内容和所有评论
将评论整理成一个表格，包含author、content和time列
从评论内容中提取所有链接
使用webshot2包访问链接并生成网页截图
将截图保存在webshot文件夹中
将截图的文件路径添加到表格中
将结果保存为CSV文件
使用方法
将代码保存为github_issue_parser.R
修改代码中的repo_owner和repo_name变量，设置为你要爬取的GitHub仓库
运行脚本
注意：

使用此脚本需要安装Chrome或Chromium浏览器，因为webshot2包依赖它来生成截图
对于私有仓库，你可能需要设置GitHub Personal Access Token
如果遇到API速率限制，可能需要增加等待时间或减少请求频率