# 处理回复

创建一个R脚本，用于从 GitHub Issue [#1](https://github.com/D2RS-2025spring/myHomePages/issues/1) 中提取回复，整理为表格，并对内容中的链接生成网页截图。

## 准备工作

首先，我们需要安装并加载一些必要的R包，包括 `gh`、`webshot2`、`dplyr`、`stringr`、`tibble`、`httr` 和 `jsonlite`。如果你尚未安装这些包，可以运行以下代码来安装它们：

```{r}
# 安装必要的包（如果尚未安装）
if (!require("gh", quietly = TRUE)) install.packages("gh")
if (!require("webshot2", quietly = TRUE)) install.packages("webshot2")
if (!require("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!require("stringr", quietly = TRUE)) install.packages("stringr")
if (!require("tibble", quietly = TRUE)) install.packages("tibble")
if (!require("httr", quietly = TRUE)) install.packages("httr")
if (!require("jsonlite", quietly = TRUE)) install.packages("jsonlite")

# 加载包
library(gh)
library(webshot2)
library(dplyr)
library(stringr)
library(tibble)
library(httr)
library(jsonlite)

# 创建webshot文件夹（如果不存在）
if (!dir.exists("webshot")) {
  dir.create("webshot")
}
```

这些包的作用如下：

* `gh`：用于访问 GitHub API
* `webshot2`：用于生成网页截图
* `dplyr`：用于数据处理
* `stringr`：用于字符串处理
* `tibble`：用于创建数据框
* `httr`：用于发起 HTTP 请求
* `jsonlite`：用于处理 JSON 数据

## 获取评论

接下来，我们将使用 GitHub API 获取指定仓库的 Issue #1 的所有评论。

首先，将以下代码中的 `repo_owner`、`repo_name` 和 `issue_number` 替换为要爬取的仓库和 Issue 编号：


```{r}
# 指定仓库和Issue编号
# 请替换为你需要爬取的仓库
repo_owner <- "D2RS-2025spring"
repo_name <- "myHomePages"
issue_number <- 1

# 使用GitHub API获取issue评论
issue_comments <- gh::gh(
  "GET /repos/{owner}/{repo}/issues/{issue_number}/comments",
  owner = repo_owner,
  repo = repo_name,
  issue_number = issue_number,
  .limit = Inf
)
```

在这里使用 `.limit = Inf` 来获取所有评论，如果评论数量较多，可能需要等待一段时间。

## 打印原始 Issue 内容

在获取 ISSUE comments 的方法中，并不会包含第一条的评论（即 Issue 的标题和内容），所以我们需要单独获取 Issue 的标题和内容，打印原始 Issue 的标题和内容，以便了解 Issue 的背景信息。

```{r}
# 打印原始issue内容

# 获取原始issue内容
issue_data <- gh::gh(
  "GET /repos/{owner}/{repo}/issues/{issue_number}",
  owner = repo_owner,
  repo = repo_name,
  issue_number = issue_number
)

cli::cat_rule("原始Issue内容")
cat(issue_data$title, "\n")
cat(issue_data$body, "\n")
```

## 整理评论

利用 `lapply` 函数将评论整理为一个列表，然后使用 `bind_rows` 函数将列表转换为 tibble，方便后续处理。

```{r}
# 创建一个列表来存储评论信息
comments_list = lapply(issue_comments, function(comment) {
  tibble(
    author = comment$user$login,
    content = comment$body,
    content_url = comment$html_url,
    time = comment$created_at,
    id = NA,
    links = NA,
    screenshot_paths = NA
  )
})

# 将列表转换为 tibble
comments_df = bind_rows(comments_list)

comments_df
```

`comments_df` 是一个数据框，包含了评论的作者、内容、评论链接和时间。

## 提取评论中的学号

从评论内容中提取学号。这里使用了正则表达式来匹配学号，然后将提取的学号保存到数据框中。

这里使用的正则表达式为 `(?<!\\d)\\d{13}(?!\\d)`，可以匹配 13 位数字，前后不是数字的情况。其中 `(?<!\\d)` 表示前面不是数字，`\\d{13}` 表示匹配 13 位数字，`(?!\\d)` 表示后面不是数字。

```{r}
# 提取每条评论中的学号
for (i in 1:nrow(comments_df)) {
  # 使用正则表达式提取学号
  # 这个模式可以匹配学号
  id_pattern <- "(?<!\\d)\\d{13}(?!\\d)"
  matches <- str_extract_all(comments_df$content[i], id_pattern)[[1]]
  matches <- unique(matches)
  id <- NA
  if (length(matches) < 1) {
    # 如果没有匹配，打印警告
    warning(glue::glue("评论 {i} 中未检测到学号: {comments_df$content[i]}"))
  } else if (length(matches) > 1) {
    # 如果有多个匹配，只取第一个
    id <- matches[1]
    message(glue::glue("评论 {i} 中有多个学号匹配: `{comments_df$content[i]}`。只取第一个: {id}"))
  } else {
    # 如果只有一个匹配，直接取出
    id = matches[1]
  }
  # 更新数据框
  comments_df$id[[i]]<- id
}

# 检查学号提取是否正确
comments_df
```


## 提取评论中的链接

从评论内容中提取可用的链接。这里使用了正则表达式来匹配链接，包括 Markdown 风格的链接和直接 URL。然后将提取的链接保存到数据框中。

这里使用的正则表达式为 `https?://[^\\s()<>\"\\[\\]]+|\\(https?://[^\\s()<>\"\\[\\]]+\\)|\\[.*?\\]\\(https?://[^\\s()<>\"\\[\\]]+\\)`，可以匹配 Markdown 风格的链接和直接 URL。其中，`https?://[^\\s()<>\"\\[\\]]+` 匹配直接 URL，`\\(https?://[^\\s()<>\"\\[\\]]+\\)` 匹配圆括号中的链接，`\\[.*?\\]\\(https?://[^\\s()<>\"\\[\\]]+\\)` 匹配 Markdown 风格的链接。

```{r}
# 提取每条评论中的链接
for (i in 1:nrow(comments_df)) {
  # 使用正则表达式提取链接
  # 这个模式可以匹配Markdown风格的链接和直接URL
  url_pattern <- "https?://[^\\s()<>\"\\[\\]]+|\\(https?://[^\\s()<>\"\\[\\]]+\\)|\\[.*?\\]\\(https?://[^\\s()<>\"\\[\\]]+\\)"
  matches <- str_extract_all(comments_df$content[i], url_pattern)[[1]]
  
  # 清理链接（从Markdown格式中提取实际URL）
  cleaned_links <- character()
  for (match in matches) {
    # 如果是Markdown格式的链接 [text](url)
    if (str_detect(match, "\\[.*?\\]\\(.*?\\)")) {
      url <- str_extract(match, "(?<=\\().*?(?=\\))")
      cleaned_links <- c(cleaned_links, url)
    } 
    # 如果是圆括号中的链接 (url)
    else if (str_detect(match, "^\\(.*?\\)$")) {
      url <- str_sub(match, 2, -2)
      cleaned_links <- c(cleaned_links, url)
    } 
    # 直接URL
    else {
      cleaned_links <- c(cleaned_links, match)
    }
  }
  
  # 过滤掉非HTTP链接
  valid_links <- cleaned_links[str_detect(cleaned_links, "^https?://")]
  
  # 更新数据框
  comments_df$links[i] <- list(valid_links)
  
}
```


检查链接提取是否正确，并打印出每条评论中的内容。

```{r}
# 没有提取到链接的评论链接
comments_df |> 
    rowwise() |>
    filter(length(links) == 0) |>
    select(author, content)
```


## 生成网页截图

使用 `webshot2` 包生成评论中的链接的网页截图，并将截图保存到 `webshot` 文件夹中。然后将截图的文件路径添加到数据框中。

```{r}
# 使用webshot2包生成网页截图
for (i in 1:nrow(comments_df)) {
  valid_links <- comments_df$links[[i]]
  # 准备保存截图路径
  screenshot_paths <- character()
  
  # 为每个有效链接生成截图
  if (length(valid_links) > 0) {
    for (j in 1:length(valid_links)) {
      link <- valid_links[j]
      # 创建一个安全的文件名
      safe_filename <- paste0("webshot/", 
                               i, "_", j, "_", 
                               str_replace_all(gsub("https?://", "", link), "[^a-zA-Z0-9]", "_"), 
                               ".png")
      
      # 如果文件已经存在，跳过
        if (file.exists(safe_filename) & file.size(safe_filename) > 1000) {
            message(sprintf("跳过截图: %s (文件已存在)\n", link))
            screenshot_paths <- c(screenshot_paths, safe_filename)
            next
        }
      
      # 尝试截图
      tryCatch({
        webshot2::webshot(url = link, 
                          file = safe_filename, 
                          delay = 5)
        screenshot_paths <- c(screenshot_paths, safe_filename)
        # message(sprintf("成功截图: %s\n", link))
      }, error = function(e) {
        warning(sprintf("截图失败: %s, 错误: %s\n", link, e$message))
        screenshot_paths <- c(screenshot_paths, NA)
      })
    }
  }
  
  # 更新截图路径
  comments_df$screenshot_paths[[i]] <- list(screenshot_paths)
}
```


## 保存结果

最后，将整理好的评论数据保存为CSV文件，并输出处理结果。对于存在多个链接的评论（有些同学提交了多个版本），将多个链接用分号分隔，合并为一个评论。

```{r}
# 将 list 转换为字符向量
final_df <- comments_df %>%
  mutate(
    links = sapply(links, function(x) paste(x, collapse = "; ")),
    screenshot_paths = sapply(screenshot_paths, function(x) paste(x, collapse = "; "))
  )

# 保存结果到CSV
write.csv(final_df, "github_issue_comments.csv", row.names = FALSE, fileEncoding = "UTF-8")

# 输出结果
cat("处理完成! 共处理了", nrow(comments_df), "条评论\n")
cat("结果已保存到 github_issue_comments.csv\n")
print(final_df)
```


## 小结


这个 R 脚本会完成以下任务：

* 安装并加载必要的R包
* 创建一个 webshot 文件夹用于保存网页截图
* 从 GitHub API 获取 Issue #1 的内容和所有评论
* 将评论整理成一个表格，包含 `author`、`content` 和 `time` 列等
* 从评论内容中提取所有链接和学号
* 使用 `webshot2` 包访问链接并生成网页截图
* 将截图保存在 `webshot` 文件夹中
* 将截图的文件路径添加到表格中
* 将结果保存为CSV文件
