[
  {
    "objectID": "shiny-test3.html",
    "href": "shiny-test3.html",
    "title": "网页秀和投票",
    "section": "",
    "text": "用 R 实现下面的功能：\n\n读取 GitHub Issues；\n使用网页截图生成一个分页的展示栏，提供3种排序方式，按照发布时间、按照id、按照点赞数；\n基于 GitHub ISSUE 的点赞功能实现点赞。每个网页截图的点赞添加到content_url所指的评论上。\n\n接下来，让我们一步一步地用 R 实现这个功能。\n\n\n首先，我们需要安装和加载一些 R 包，用于获取 GitHub Issue 数据、生成网页截图、处理数据等。\n\n# 安装和加载必要的 R 包\nif (!require(\"gh\", quietly = TRUE)) install.packages(\"gh\")\nif (!require(\"webshot2\", quietly = TRUE)) install.packages(\"webshot2\")\nif (!require(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\nif (!require(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\nif (!require(\"tibble\", quietly = TRUE)) install.packages(\"tibble\")\nif (!require(\"httr\", quietly = TRUE)) install.packages(\"httr\")\nif (!require(\"jsonlite\", quietly = TRUE)) install.packages(\"jsonlite\")\n\n# 加载 R 包\nlibrary(gh)\nlibrary(webshot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(httr)\nlibrary(jsonlite)\n\n\n\n\nget() 函数用于从评论内容中提取匹配的内容。如果没有匹配，会打印警告；如果有多个匹配，会保留第一个；如果只有一个匹配，直接返回。\nget_id() 这里使用的正则表达式为 (?&lt;!\\\\d)\\\\d{13}(?!\\\\d)，可以匹配 13 位数字，前后不是数字的情况。其中 (?&lt;!\\\\d) 表示前面不是数字，\\\\d{13} 表示匹配 13 位数字，(?!\\\\d) 表示后面不是数字。\nget_link() 这里使用的正则表达式为 https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+，可以匹配直接 URL。其中 https?:// 表示匹配 http:// 或 https://，[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+ 表示匹配除空格、括号、尖括号、引号和方括号之外的字符。\ncreate_webshot() 用于生成网页截图，如果文件已经存在且大小大于 1 kb，则跳过。如果文件不存在或大小小于 1 kb，则尝试生成网页截图。如果生成成功，则打印成功信息；如果生成失败，则打印失败信息。\n\nget = function(content, pattern){\n    if (!str_detect(content, pattern)) {\n        warning(glue::glue(\"{content} 中未检测到: {pattern}\"))\n        return(NULL)\n    }\n    matches &lt;- str_extract_all(content, pattern)[[1]]\n    matches &lt;- unique(matches)\n    match &lt;- NULL\n    if (length(matches) &gt; 1) {\n        # 如果有多个匹配，只取第一个\n        match &lt;- matches[1]\n        message(glue::glue(\"{content} 中检测到多个匹配: `{pattern}`。保留第一个。\"))\n    } else {\n        # 如果只有一个匹配，直接取出\n        match = matches[1]\n    }\n    return(match)\n}\n\nget_id = function(content){\n    get(content, pattern = \"(?&lt;!\\\\d)\\\\d{13}(?!\\\\d)\")\n}\n\nget_link = function(content){\n    get(content, pattern = \"https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+\")\n}\n\ncreate_webshot = function(link, verbose = TRUE) {\n    # valid link\n    if (is.null(link) || !str_detect(link, \"^https?://\")) {\n        warning(glue::glue(\"Invalid link: {link}\"))\n        return(NULL)\n    }\n\n    # 创建一个安全的文件名\n    safe_filename &lt;- paste0(\"www/webshot/\", \n                            str_replace_all(gsub(\"https?://\", \"\", link), \"[^a-zA-Z0-9]\", \"_\"), \n                            \".png\")\n    \n    # 如果文件已经存在且大小大于 1 kb，跳过\n    if (file.exists(safe_filename) & file.size(safe_filename) &gt; 1000) {\n        if (verbose) message(sprintf(\"跳过截图: %s (文件已存在)\\n\", link))\n    } else {\n        # 尝试截图\n        tryCatch({\n            webshot2::webshot(url = link, \n                                file = safe_filename, \n                                delay = 5)\n            if (verbose) message(sprintf(\"成功截图: %s\\n\", link))\n        }, error = function(e) {\n            if (verbose) warning(sprintf(\"截图失败: %s, 错误: %s\\n\", link, e$message))\n        })\n    }\n    \n    safe_filename = gsub(\"www/\", \"\", safe_filename)\n    return(safe_file\n}\n\nparse_issue = function(issue){\n    # 提取评论信息\n    user_id = issue$user$id\n    user_name = issue$user$login\n    content = str_c(issue$title, issue$body, sep = \"\\n\") # 标题和内容合并\n    content_url = issue$html_url\n    time = issue$created_at\n    student_id = get_id(content)\n    student_homepage_link = get_link(content)\n    # student_homepage_webshot = create_webshot(student_homepage_link, verbose = FALSE)\n    reaction_url = issue$reactions$url\n    reaction_total_count = issue$reactions$total_count\n\n    # 返回一个 tibble\n    tibble(\n        user_name = user_name,\n        user_id = user_id,\n        content = content,\n        content_url = content_url,\n        time = time,\n        student_id = student_id,\n        student_homepage_link = student_homepage_link,\n        # student_homepage_webshot = student_homepage_webshot,\n        reaction_url = reaction_url,\n        reaction_total_count = reaction_total_count\n    )\n}\n\n\n\n\n有些同学不按要求提交，新开了 Issue 提交作业，甚至在 Issue 中回复分开提交链接和学号，给处理数据带来了很大困难。\n\n获取所有 Issue；\n获取所有 Issue 的 comments；\n将这些结果合并在一起。\n\n\n# 指定仓库\nrepo_owner &lt;- \"D2RS-2025spring\"\nrepo_name &lt;- \"myHomePages\"\n\n# 使用 GitHub API 获取指定仓库的所有 Issue 数据\nissues &lt;- gh::gh(\n  \"GET /repos/{owner}/{repo}/issues\",\n  owner = repo_owner,\n  repo = repo_name\n)\n\nissue_data = lapply(issues, parse_issue)  |&gt;  bind_rows(issue_data)"
  },
  {
    "objectID": "shiny-test3.html#安装和加载必要的-r-包",
    "href": "shiny-test3.html#安装和加载必要的-r-包",
    "title": "网页秀和投票",
    "section": "",
    "text": "首先，我们需要安装和加载一些 R 包，用于获取 GitHub Issue 数据、生成网页截图、处理数据等。\n\n# 安装和加载必要的 R 包\nif (!require(\"gh\", quietly = TRUE)) install.packages(\"gh\")\nif (!require(\"webshot2\", quietly = TRUE)) install.packages(\"webshot2\")\nif (!require(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\nif (!require(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\nif (!require(\"tibble\", quietly = TRUE)) install.packages(\"tibble\")\nif (!require(\"httr\", quietly = TRUE)) install.packages(\"httr\")\nif (!require(\"jsonlite\", quietly = TRUE)) install.packages(\"jsonlite\")\n\n# 加载 R 包\nlibrary(gh)\nlibrary(webshot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(httr)\nlibrary(jsonlite)"
  },
  {
    "objectID": "shiny-test3.html#创建辅助函数",
    "href": "shiny-test3.html#创建辅助函数",
    "title": "网页秀和投票",
    "section": "",
    "text": "get() 函数用于从评论内容中提取匹配的内容。如果没有匹配，会打印警告；如果有多个匹配，会保留第一个；如果只有一个匹配，直接返回。\nget_id() 这里使用的正则表达式为 (?&lt;!\\\\d)\\\\d{13}(?!\\\\d)，可以匹配 13 位数字，前后不是数字的情况。其中 (?&lt;!\\\\d) 表示前面不是数字，\\\\d{13} 表示匹配 13 位数字，(?!\\\\d) 表示后面不是数字。\nget_link() 这里使用的正则表达式为 https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+，可以匹配直接 URL。其中 https?:// 表示匹配 http:// 或 https://，[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+ 表示匹配除空格、括号、尖括号、引号和方括号之外的字符。\ncreate_webshot() 用于生成网页截图，如果文件已经存在且大小大于 1 kb，则跳过。如果文件不存在或大小小于 1 kb，则尝试生成网页截图。如果生成成功，则打印成功信息；如果生成失败，则打印失败信息。\n\nget = function(content, pattern){\n    if (!str_detect(content, pattern)) {\n        warning(glue::glue(\"{content} 中未检测到: {pattern}\"))\n        return(NULL)\n    }\n    matches &lt;- str_extract_all(content, pattern)[[1]]\n    matches &lt;- unique(matches)\n    match &lt;- NULL\n    if (length(matches) &gt; 1) {\n        # 如果有多个匹配，只取第一个\n        match &lt;- matches[1]\n        message(glue::glue(\"{content} 中检测到多个匹配: `{pattern}`。保留第一个。\"))\n    } else {\n        # 如果只有一个匹配，直接取出\n        match = matches[1]\n    }\n    return(match)\n}\n\nget_id = function(content){\n    get(content, pattern = \"(?&lt;!\\\\d)\\\\d{13}(?!\\\\d)\")\n}\n\nget_link = function(content){\n    get(content, pattern = \"https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+\")\n}\n\ncreate_webshot = function(link, verbose = TRUE) {\n    # valid link\n    if (is.null(link) || !str_detect(link, \"^https?://\")) {\n        warning(glue::glue(\"Invalid link: {link}\"))\n        return(NULL)\n    }\n\n    # 创建一个安全的文件名\n    safe_filename &lt;- paste0(\"www/webshot/\", \n                            str_replace_all(gsub(\"https?://\", \"\", link), \"[^a-zA-Z0-9]\", \"_\"), \n                            \".png\")\n    \n    # 如果文件已经存在且大小大于 1 kb，跳过\n    if (file.exists(safe_filename) & file.size(safe_filename) &gt; 1000) {\n        if (verbose) message(sprintf(\"跳过截图: %s (文件已存在)\\n\", link))\n    } else {\n        # 尝试截图\n        tryCatch({\n            webshot2::webshot(url = link, \n                                file = safe_filename, \n                                delay = 5)\n            if (verbose) message(sprintf(\"成功截图: %s\\n\", link))\n        }, error = function(e) {\n            if (verbose) warning(sprintf(\"截图失败: %s, 错误: %s\\n\", link, e$message))\n        })\n    }\n    \n    safe_filename = gsub(\"www/\", \"\", safe_filename)\n    return(safe_file\n}\n\nparse_issue = function(issue){\n    # 提取评论信息\n    user_id = issue$user$id\n    user_name = issue$user$login\n    content = str_c(issue$title, issue$body, sep = \"\\n\") # 标题和内容合并\n    content_url = issue$html_url\n    time = issue$created_at\n    student_id = get_id(content)\n    student_homepage_link = get_link(content)\n    # student_homepage_webshot = create_webshot(student_homepage_link, verbose = FALSE)\n    reaction_url = issue$reactions$url\n    reaction_total_count = issue$reactions$total_count\n\n    # 返回一个 tibble\n    tibble(\n        user_name = user_name,\n        user_id = user_id,\n        content = content,\n        content_url = content_url,\n        time = time,\n        student_id = student_id,\n        student_homepage_link = student_homepage_link,\n        # student_homepage_webshot = student_homepage_webshot,\n        reaction_url = reaction_url,\n        reaction_total_count = reaction_total_count\n    )\n}"
  },
  {
    "objectID": "shiny-test3.html#获取-github-issue-数据",
    "href": "shiny-test3.html#获取-github-issue-数据",
    "title": "网页秀和投票",
    "section": "",
    "text": "有些同学不按要求提交，新开了 Issue 提交作业，甚至在 Issue 中回复分开提交链接和学号，给处理数据带来了很大困难。\n\n获取所有 Issue；\n获取所有 Issue 的 comments；\n将这些结果合并在一起。\n\n\n# 指定仓库\nrepo_owner &lt;- \"D2RS-2025spring\"\nrepo_name &lt;- \"myHomePages\"\n\n# 使用 GitHub API 获取指定仓库的所有 Issue 数据\nissues &lt;- gh::gh(\n  \"GET /repos/{owner}/{repo}/issues\",\n  owner = repo_owner,\n  repo = repo_name\n)\n\nissue_data = lapply(issues, parse_issue)  |&gt;  bind_rows(issue_data)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "网页秀",
    "section": "",
    "text": "网页秀\n创建网页截图，并展示在网页上。\n\n\n\n  \n  \n    mortalccookiee\n  \n  \n    \n  \n\n\n  \n    MKS-1120\n  \n  \n    \n  \n\n\n  \n    Lvicky13\n  \n  \n    \n  \n\n\n  \n    songllan\n  \n  \n    \n  \n\n\n  \n    songllan\n  \n  \n    \n  \n\n\n  \n    huaarui\n  \n  \n    \n  \n\n\n  \n    songllan\n  \n  \n    \n  \n\n\n  \n    AlcottGiovanna\n  \n  \n    \n  \n\n\n  \n    YNlT\n  \n  \n    \n  \n\n\n  \n    muqiyan\n  \n  \n    \n  \n\n\n  \n    6-NanCheng\n  \n  \n    \n  \n\n\n  \n    sas12-bot\n  \n  \n    \n  \n\n\n  \n    lmc157311\n  \n  \n    \n  \n\n\n  \n    vidool\n  \n  \n    \n  \n\n\n  \n    Z88-ZMQ\n  \n  \n    \n  \n\n\n  \n    2024303110046\n  \n  \n    \n  \n\n\n  \n    unrestraint1\n  \n  \n    \n  \n\n\n  \n    ZhuzhPhloem\n  \n  \n    \n  \n\n\n  \n    cheng-xubo\n  \n  \n    \n  \n\n\n  \n    Liting-shu\n  \n  \n    \n  \n\n\n  \n    BenLester16\n  \n  \n    \n  \n\n\n  \n    jayzky\n  \n  \n    \n  \n\n\n  \n    NTThome\n  \n  \n    \n  \n\n\n  \n    chentaishuai\n  \n  \n    \n  \n\n\n  \n    77-cute\n  \n  \n    \n  \n\n\n  \n    Carolineno\n  \n  \n    \n  \n\n\n  \n    Liuhai626\n  \n  \n    \n  \n\n\n  \n    Wang-CJ688\n  \n  \n    \n  \n\n\n  \n    lixinaaaaaa\n  \n  \n    \n  \n\n\n  \n    aidenlee157\n  \n  \n    \n  \n\n\n  \n    xiao666-Star\n  \n  \n    \n  \n\n\n  \n    xiaoziling24\n  \n  \n    \n  \n\n\n  \n    GG10BOND\n  \n  \n    \n  \n\n\n  \n    maixiantao\n  \n  \n    \n  \n\n\n  \n    zhangyuandorr\n  \n  \n    \n  \n\n\n  \n    kyrie831\n  \n  \n    \n  \n\n\n  \n    xixi123-gif\n  \n  \n    \n  \n\n\n  \n    llffgihnmnb\n  \n  \n    \n  \n\n\n  \n    xx778\n  \n  \n    \n  \n\n\n  \n    zrthuihb\n  \n  \n    \n  \n\n\n  \n    Xinke-dot\n  \n  \n    \n  \n\n\n  \n    MXY-star\n  \n  \n    \n  \n\n\n  \n    wsygxyl\n  \n  \n    \n  \n\n\n  \n    zjysll\n  \n  \n    \n  \n\n\n  \n    YanWanYing\n  \n  \n    \n  \n\n\n  \n    wxq-xqw\n  \n  \n    \n  \n\n\n  \n    ydk-123\n  \n  \n    \n  \n\n\n  \n    orone5\n  \n  \n    \n  \n\n\n  \n    Tricky-0007\n  \n  \n    \n  \n\n\n  \n    tangjiaxin-web\n  \n  \n    \n  \n\n\n  \n    wwh333\n  \n  \n    \n  \n\n\n  \n    dengjun667\n  \n  \n    \n  \n\n\n  \n    notlikepear\n  \n  \n    \n  \n\n\n  \n    14hg-D\n  \n  \n    \n  \n\n\n  \n    xmaql\n  \n  \n    \n  \n\n\n  \n    Ffffffai\n  \n  \n    \n  \n\n\n  \n    yuyuyuyuyu222\n  \n  \n    \n  \n\n\n  \n    Blue-sir-1\n  \n  \n    \n  \n\n\n  \n    shupian123\n  \n  \n    \n  \n\n\n  \n    guo101-ux\n  \n  \n    \n  \n\n\n  \n    Songjia12\n  \n  \n    \n  \n\n\n  \n    Emmadev475\n  \n  \n    \n  \n\n\n  \n    eybb25\n  \n  \n    \n  \n\n\n  \n    R-yf\n  \n  \n    \n  \n\n\n  \n    hexinyu917\n  \n  \n    \n  \n\n\n  \n    Zouliii\n  \n  \n    \n  \n\n\n  \n    chenxiuying123\n  \n  \n    \n  \n\n\n  \n    eatch16\n  \n  \n    \n  \n\n\n  \n    1778-ctrl\n  \n  \n    \n  \n\n\n  \n    Wushanzhi\n  \n  \n    \n  \n\n\n  \n    xzs16\n  \n  \n    \n  \n\n\n  \n    wss0910\n  \n  \n    \n  \n\n\n  \n    WJ011209\n  \n  \n    \n  \n\n\n  \n    manman925\n  \n  \n    \n  \n\n\n  \n    syz357\n  \n  \n    \n  \n\n\n  \n    ningcaier\n  \n  \n    \n  \n\n\n  \n    yoayoayoa\n  \n  \n    \n  \n\n\n  \n    Cxy3207\n  \n  \n    \n  \n\n\n  \n    ha-hug\n  \n  \n    \n  \n\n\n  \n    usggvv\n  \n  \n    \n  \n\n\n  \n    chuwenyu\n  \n  \n    \n  \n\n\n  \n    mengtianhuang\n  \n  \n    \n  \n\n\n  \n    Zhang-Huan527\n  \n  \n    \n  \n\n\n  \n    jing3-xu\n  \n  \n    \n  \n\n\n  \n    ASFSADGAD\n  \n  \n    \n  \n\n\n  \n    Spade-Atek\n  \n  \n    \n  \n\n\n  \n    JT-0222\n  \n  \n    \n  \n\n\n  \n    777Pu\n  \n  \n    \n  \n\n\n  \n    crx2024\n  \n  \n    \n  \n\n\n  \n    yql2025\n  \n  \n    \n  \n\n\n  \n    sxy0326\n  \n  \n    \n  \n\n\n  \n    Ouyang123456175\n  \n  \n    \n  \n\n\n  \n    yinqi2024303120023\n  \n  \n    \n  \n\n\n  \n    duke123-afk\n  \n  \n    \n  \n\n\n  \n    abc2288\n  \n  \n    \n  \n\n\n  \n    123S-OSS\n  \n  \n    \n  \n\n\n  \n    ZOUYANG520\n  \n  \n    \n  \n\n\n  \n    wangjiangpeng489\n  \n  \n    \n  \n\n\n  \n    GDL0909\n  \n  \n    \n  \n\n\n  \n    zzss1204\n  \n  \n    \n  \n\n\n  \n    Jiangbin-del\n  \n  \n    \n  \n\n\n  \n    WeiduJ\n  \n  \n    \n  \n\n\n  \n    LYCOOLL\n  \n  \n    \n  \n\n\n  \n    jiayi-bit\n  \n  \n    \n  \n\n\n  \n    yans111111\n  \n  \n    \n  \n\n\n  \n    shifazhen\n  \n  \n    \n  \n\n\n  \n    hyt702\n  \n  \n    \n  \n\n\n  \n    YZ-999\n  \n  \n    \n  \n\n\n  \n    MOCCKING\n  \n  \n    \n  \n\n\n  \n    G-1988\n  \n  \n    \n  \n\n\n  \n    xi-kexin\n  \n  \n    \n  \n\n\n  \n    liukaiyue-create\n  \n  \n    \n  \n\n\n  \n    ccy-30\n  \n  \n    \n  \n\n\n  \n    liukaiyue-create\n  \n  \n    \n  \n\n\n  \n    lllfy666\n  \n  \n    \n  \n\n\n  \n    hsywww\n  \n  \n    \n  \n\n\n  \n    q1235qq\n  \n  \n    \n  \n\n\n  \n    KKXLNDS23\n  \n  \n    \n  \n\n\n  \n    jiang065\n  \n  \n    \n  \n\n\n  \n    ggggmy\n  \n  \n    \n  \n\n\n  \n    xiaoiyn\n  \n  \n    \n  \n\n\n  \n    good305\n  \n  \n    \n  \n\n\n  \n    fmh2025\n  \n  \n    \n  \n\n\n  \n    Liuhai626\n  \n  \n    \n  \n\n\n  \n    an0070\n  \n  \n    \n  \n\n\n  \n    yangyangyang-chen\n  \n  \n    \n  \n\n\n  \n    buchi245\n  \n  \n    \n  \n\n\n  \n    yv0610\n  \n  \n    \n  \n\n\n  \n    jung-7-beep\n  \n  \n    \n  \n\n\n  \n    xuexue-chen\n  \n  \n    \n  \n\n\n  \n    xunxuna66\n  \n  \n    \n  \n\n\n  \n    Xin-959\n  \n  \n    \n  \n\n\n  \n    libee7864\n  \n  \n    \n  \n\n\n  \n    emma88887\n  \n  \n    \n  \n\n\n  \n    weekryan\n  \n  \n    \n  \n\n\n  \n    lyf123456654321\n  \n  \n    \n  \n\n\n  \n    SunM-lab\n  \n  \n    \n  \n\n\n  \n    djl18461075977\n  \n  \n    \n  \n\n\n  \n    HXL5151\n  \n  \n    \n  \n\n\n  \n    LeoryWang\n  \n  \n    \n  \n\n\n  \n    liyalin0811\n  \n  \n    \n  \n\n\n  \n    coraluo\n  \n  \n    \n  \n\n\n  \n    hzauym\n  \n  \n    \n  \n\n\n  \n    acdmini\n  \n  \n    \n  \n\n\n  \n    whx-321\n  \n  \n    \n  \n\n\n  \n    huihuihui-ma\n  \n  \n    \n  \n\n\n  \n    Zhang1hzau\n  \n  \n    \n  \n\n\n  \n    libee7864\n  \n  \n    \n  \n\n\n  \n    caoxuhui1\n  \n  \n    \n  \n\n\n  \n    Pyq-bit\n  \n  \n    \n  \n\n\n  \n    mengchu-chu\n  \n  \n    \n  \n\n\n  \n    Penglijuanxxx\n  \n  \n    \n  \n\n\n  \n    LS1457\n  \n  \n    \n  \n\n\n  \n    7352Mengyuan\n  \n  \n    \n  \n\n\n  \n    chengchaofan-123\n  \n  \n    \n  \n\n\n  \n    DongKuankk\n  \n  \n    \n  \n\n\n  \n    Zhang1hzau\n  \n  \n    \n  \n\n\n  \n    SSY-DEL\n  \n  \n    \n  \n\n\n  \n    sqzhang400\n  \n  \n    \n  \n\n\n  \n    Francescalulu\n  \n  \n    \n  \n\n\n  \n    fansongwei-dev\n  \n  \n    \n  \n\n\n  \n    luluyue\n  \n  \n    \n  \n\n\n  \n    huq-q\n  \n  \n    \n  \n\n\n  \n    zxy110031\n  \n  \n    \n  \n\n\n  \n    qazzjh\n  \n  \n    \n  \n\n\n  \n    honeymie\n  \n  \n    \n  \n\n\n  \n    chenzhihan-web\n  \n  \n    \n  \n\n\n  \n    rdzhou-2002\n  \n  \n    \n  \n\n\n  \n    simplelity\n  \n  \n    \n  \n\n\n  \n    yxs-plj\n  \n  \n    \n  \n\n\n  \n    shen-cmyk\n  \n  \n    \n  \n\n\n  \n    Amerhan1\n  \n  \n    \n  \n\n\n  \n    ranqin-qu\n  \n  \n    \n  \n\n\n  \n    GAOMING-RUI\n  \n  \n    \n  \n\n\n  \n    yanbohan889\n  \n  \n    \n  \n\n\n  \n    pp285\n  \n  \n    \n  \n\n\n  \n    ykyh28\n  \n  \n    \n  \n\n\n  \n    JYQ-123\n  \n  \n    \n  \n\n\n  \n    zhou125910\n  \n  \n    \n  \n\n\n  \n    zhaoyaoyao423\n  \n  \n    \n  \n\n\n  \n    weare-lab\n  \n  \n    \n  \n\n\n  \n    3012710582\n  \n  \n    \n  \n\n\n  \n    Lyx-hub789\n  \n  \n    \n  \n\n\n  \n    ZBFHJ\n  \n  \n    \n  \n\n\n  \n    2024303120063\n  \n  \n    \n  \n\n\n  \n    m1st0917\n  \n  \n    \n  \n\n\n  \n    Xiongmaowang14\n  \n  \n    \n  \n\n\n  \n    2024303120063\n  \n  \n    \n  \n\n\n  \n    Ming1209x\n  \n  \n    \n  \n\n\n  \n    yufan-19960521\n  \n  \n    \n  \n\n\n  \n    JoostLiu11\n  \n  \n    \n  \n\n\n  \n    qianqiu-sos\n  \n  \n    \n  \n\n\n  \n    pmk1017\n  \n  \n    \n  \n\n\n  \n    heleling\n  \n  \n    \n  \n\n\n  \n    Zhang2024303110032\n  \n  \n    \n  \n\n\n  \n    Song-1228-snow\n  \n  \n    \n  \n\n\n  \n    ShenYx16\n  \n  \n    \n  \n\n\n  \n    xlw911\n  \n  \n    \n  \n\n\n  \n    lumos-0\n  \n  \n    \n  \n\n\n  \n    aili566091\n  \n  \n    \n  \n\n\n  \n    t12ing\n  \n  \n    \n  \n\n\n  \n    YEE-Caroline\n  \n  \n    \n  \n\n\n  \n    er495\n  \n  \n    \n  \n\n\n  \n    vvjpbb\n  \n  \n    \n  \n\n\n  \n    WJX2024303110022\n  \n  \n    \n  \n\n\n  \n    zj303110034\n  \n  \n    \n  \n\n\n  \n    chen5210311\n  \n  \n    \n  \n\n\n  \n    zwz1004\n  \n  \n    \n  \n\n\n  \n    qyw111\n  \n  \n    \n  \n\n\n  \n    kevin-1225\n  \n  \n    \n  \n\n\n  \n    chen5210311\n  \n  \n    \n  \n\n\n  \n    zkx-bit123\n  \n  \n    \n  \n\n\n  \n    SNOW0311\n  \n  \n    \n  \n\n\n  \n    la0626\n  \n  \n    \n  \n\n\n  \n    haiyu-18\n  \n  \n    \n  \n\n\n  \n    11-sansb\n  \n  \n    \n  \n\n\n  \n    Vendy985\n  \n  \n    \n  \n\n\n  \n    Cathyhm\n  \n  \n    \n  \n\n\n  \n    q1235qq\n  \n  \n    \n  \n\n\n  \n    marlon721\n  \n  \n    \n  \n\n\n  \n    Song-1228-snow\n  \n  \n    \n  \n\n\n  \n    Cathyhm\n  \n  \n    \n  \n\n\n  \n    SNOW0311\n  \n  \n    \n  \n\n\n  \n    lee1234567843\n  \n  \n    \n  \n\n\n  \n    tli111\n  \n  \n    \n  \n\n\n  \n    wenman123\n  \n  \n    \n  \n\n\n  \n    1322766\n  \n  \n    \n  \n\n\n  \n    ly-2025\n  \n  \n    \n  \n\n\n  \n    xuanzipei\n  \n  \n    \n  \n\n\n  \n    xiaojiangxiaoting\n  \n  \n    \n  \n\n\n  \n    jkkk47777\n  \n  \n    \n  \n\n\n  \n    caowenzhuo12\n  \n  \n    \n  \n\n\n  \n    AJ-crush\n  \n  \n    \n  \n\n\n  \n    xiaoliuxiaoliu-ux\n  \n  \n    \n  \n\n\n  \n    fas20052\n  \n  \n    \n  \n\n\n  \n    yh1511\n  \n  \n    \n  \n\n\n  \n    zym4417\n  \n  \n    \n  \n\n\n  \n    crr343\n  \n  \n    \n  \n\n\n  \n    2024303110076\n  \n  \n    \n  \n\n\n  \n    YSD418\n  \n  \n    \n  \n\n\n  \n    1900375417\n  \n  \n    \n  \n\n\n  \n    wanghaokun779\n  \n  \n    \n  \n\n\n  \n    xuexiaoshuai666\n  \n  \n    \n  \n\n\n  \n    cntsxk\n  \n  \n    \n  \n\n\n  \n    VER1212\n  \n  \n    \n  \n\n\n  \n    hunie233\n  \n  \n    \n  \n\n\n  \n    sjy7862100\n  \n  \n    \n  \n\n\n  \n    Wang-YuXuan396\n  \n  \n    \n  \n\n\n  \n    ABEL-L76\n  \n  \n    \n  \n\n\n  \n    LiLizuibang142\n  \n  \n    \n  \n\n\n  \n    xiaan-ui\n  \n  \n    \n  \n\n\n  \n    pc001hub\n  \n  \n    \n  \n\n\n  \n    mxttt173\n  \n  \n    \n  \n\n\n  \n    kk11985\n  \n  \n    \n  \n\n\n  \n    BookTotle\n  \n  \n    \n  \n\n\n  \n    DantyGure\n  \n  \n    \n  \n\n\n  \n    lxh123456-gif\n  \n  \n    \n  \n\n\n  \n    xihan-web\n  \n  \n    \n  \n\n\n  \n    liangyitao223\n  \n  \n    \n  \n\n\n  \n    honeymie\n  \n  \n    \n  \n\n\n  \n    2001wzh\n  \n  \n    \n  \n\n\n  \n    Blitzcrank08\n  \n  \n    \n  \n\n\n  \n    maoliao747\n  \n  \n    \n  \n\n\n  \n    Jun-Feng-Wu\n  \n  \n    \n  \n\n\n  \n    shuaige12581\n  \n  \n    \n  \n\n\n  \n    zcx5378\n  \n  \n    \n  \n\n\n  \n    mikk123123\n  \n  \n    \n  \n\n\n  \n    Gmyue\n  \n  \n    \n  \n\n\n  \n    liangchen693\n  \n  \n    \n  \n\n\n  \n    zhongjw110047\n  \n  \n    \n  \n\n\n  \n    yanshuyao123\n  \n  \n    \n  \n\n\n  \n    chengyonghao-cmd\n  \n  \n    \n  \n\n\n  \n    wenman123\n  \n  \n    \n  \n\n\n  \n    Zhao-kai111\n  \n  \n    \n  \n\n\n  \n    SZN1126\n  \n  \n    \n  \n\n\n  \n    Fh2356\n  \n  \n    \n  \n\n\n  \n    15-cyber\n  \n  \n    \n  \n\n\n  \n    1248879838\n  \n  \n    \n  \n\n\n  \n    1753592787\n  \n  \n    \n  \n\n\n  \n    33450\n  \n  \n    \n  \n\n\n  \n    dududuki\n  \n  \n    \n  \n\n\n  \n    hck512\n  \n  \n    \n  \n\n\n  \n    Golgi01\n  \n  \n    \n  \n\n\n  \n    mayan2333\n  \n  \n    \n  \n\n\n  \n    gongwenjie00\n  \n  \n    \n  \n\n\n  \n    Hu-huiting\n  \n  \n    \n  \n\n\n  \n    VER1212\n  \n  \n    \n  \n\n\n  \n    xiao-wang0626\n  \n  \n    \n  \n\n\n  \n    15610482510\n  \n  \n    \n  \n\n\n  \n    usggvv\n  \n  \n    \n  \n\n\n  \n    xyz-1230\n  \n  \n    \n  \n\n\n  \n    ffff271\n  \n  \n    \n  \n\n\n  \n    shusuh-pop\n  \n  \n    \n  \n\n\n  \n    Thr-02-hash\n  \n  \n    \n  \n\n\n  \n    uhhyiyi\n  \n  \n    \n  \n\n\n  \n    liyuan707\n  \n  \n    \n  \n\n\n  \n    xulili071\n  \n  \n    \n  \n\n\n  \n    GRY-123\n  \n  \n    \n  \n\n\n  \n    forever643\n  \n  \n    \n  \n\n\n  \n    John-zwz\n  \n  \n    \n  \n\n\n  \n    zhujianxin1\n  \n  \n    \n  \n\n\n  \n    zxm20020818\n  \n  \n    \n  \n\n\n  \n    ytt871258130\n  \n  \n    \n  \n\n\n  \n    Ylj0617\n  \n  \n    \n  \n\n\n  \n    kk-3-ship\n  \n  \n    \n  \n\n\n  \n    Liuqi819\n  \n  \n    \n  \n\n\n  \n    Apollo-z\n  \n  \n    \n  \n\n\n  \n    luzhaohan\n  \n  \n    \n  \n\n\n  \n    XYX020209\n  \n  \n    \n  \n\n\n  \n    shimu-zsl"
  },
  {
    "objectID": "check-test3.html",
    "href": "check-test3.html",
    "title": "检查作业提交情况",
    "section": "",
    "text": "检查作业提交情况"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "创建一个 R 脚本，用于从 GitHub Issue #1 中提取回复，整理为表格，并对内容中的链接生成网页截图。\n\n\n首先，我们需要安装并加载一些必要的R包，包括 gh、webshot2、dplyr、stringr、tibble、httr 和 jsonlite。如果你尚未安装这些包，可以运行以下代码来安装它们：\n\n# 安装必要的包（如果尚未安装）\nif (!require(\"gh\", quietly = TRUE)) install.packages(\"gh\")\nif (!require(\"webshot2\", quietly = TRUE)) install.packages(\"webshot2\")\nif (!require(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\nif (!require(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\nif (!require(\"tibble\", quietly = TRUE)) install.packages(\"tibble\")\nif (!require(\"httr\", quietly = TRUE)) install.packages(\"httr\")\nif (!require(\"jsonlite\", quietly = TRUE)) install.packages(\"jsonlite\")\n\n# 加载包\nlibrary(gh)\nlibrary(webshot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(httr)\nlibrary(jsonlite)\n\n# 创建webshot文件夹（如果不存在）\nif (!dir.exists(\"www/webshot\")) {\n  dir.create(\"www/webshot\", recursive = TRUE)\n}\n\n这些包的作用如下：\n\ngh：用于访问 GitHub API\nwebshot2：用于生成网页截图\ndplyr：用于数据处理\nstringr：用于字符串处理\ntibble：用于创建数据框\nhttr：用于发起 HTTP 请求\njsonlite：用于处理 JSON 数据\n\n\n\n\n接下来，我们将使用 GitHub API 获取指定仓库的 Issue #1 的所有评论。\n首先，将以下代码中的 repo_owner、repo_name 和 issue_number 替换为要爬取的仓库和 Issue 编号：\n\n# 指定仓库和Issue编号\n# 请替换为你需要爬取的仓库\nrepo_owner &lt;- \"D2RS-2025spring\"\nrepo_name &lt;- \"myHomePages\"\nissue_number &lt;- 1\n\n# 使用GitHub API获取issue评论\nissue_comments &lt;- gh::gh(\n  \"GET /repos/{owner}/{repo}/issues/{issue_number}/comments\",\n  owner = repo_owner,\n  repo = repo_name,\n  issue_number = issue_number,\n  .limit = Inf\n)\n\n在这里使用 .limit = Inf 来获取所有评论，如果评论数量较多，可能需要等待一段时间。\n\n\n\n在获取 ISSUE comments 的方法中，并不会包含第一条的评论（即 Issue 的标题和内容），所以我们需要单独获取 Issue 的标题和内容，打印原始 Issue 的标题和内容，以便了解 Issue 的背景信息。\n\n# 获取原始issue内容\nissue_data &lt;- gh::gh(\n  \"GET /repos/{owner}/{repo}/issues/{issue_number}\",\n  owner = repo_owner,\n  repo = repo_name,\n  issue_number = issue_number\n)\n\ncli::cat_rule(\"原始Issue内容\")\ncat(issue_data$title, \"\\n\")\ncat(issue_data$body, \"\\n\")\n\n\n\n\nget() 函数用于从评论内容中提取匹配的内容。如果没有匹配，会打印警告；如果有多个匹配，会保留第一个；如果只有一个匹配，直接返回。\nget_id() 这里使用的正则表达式为 (?&lt;!\\\\d)\\\\d{13}(?!\\\\d)，可以匹配 13 位数字，前后不是数字的情况。其中 (?&lt;!\\\\d) 表示前面不是数字，\\\\d{13} 表示匹配 13 位数字，(?!\\\\d) 表示后面不是数字。\nget_link() 这里使用的正则表达式为 https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+，可以匹配直接 URL。其中 https?:// 表示匹配 http:// 或 https://，[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+ 表示匹配除空格、括号、尖括号、引号和方括号之外的字符。\ncreate_webshot() 用于生成网页截图，如果文件已经存在且大小大于 1 kb，则跳过。如果文件不存在或大小小于 1 kb，则尝试生成网页截图。如果生成成功，则打印成功信息；如果生成失败，则打印失败信息。\n\nget = function(content, pattern){\n    matches &lt;- str_extract_all(content, pattern)[[1]]\n    matches &lt;- unique(matches)\n    match &lt;- NULL\n    if (length(matches) &lt; 1) {\n        # 如果没有匹配，打印警告\n        warning(glue::glue(\"{content} 中未检测到: {pattern}\"))\n    } else if (length(matches) &gt; 1) {\n        # 如果有多个匹配，只取第一个\n        match &lt;- matches[1]\n        message(glue::glue(\"{content} 中检测到多个匹配: `{pattern}`。保留第一个。\"))\n    } else {\n        # 如果只有一个匹配，直接取出\n        match = matches[1]\n    }\n    return(match)\n}\n\nget_id = function(content){\n    get(content, pattern = \"(?&lt;!\\\\d)\\\\d{13}(?!\\\\d)\")\n}\n\nget_link = function(content){\n    get(content, pattern = \"https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+\")\n}\n\ncreate_webshot = function(link, verbose = TRUE) {\n    # valid link\n    if (is.null(link) || !str_detect(link, \"^https?://\")) {\n        warning(glue::glue(\"Invalid link: {link}\"))\n        return(NULL)\n    }\n\n    # 创建一个安全的文件名\n    safe_filename &lt;- paste0(\"www/webshot/\", \n                            str_replace_all(gsub(\"https?://\", \"\", link), \"[^a-zA-Z0-9]\", \"_\"), \n                            \".png\")\n    \n    # 如果文件已经存在且大小大于 1 kb，跳过\n    if (file.exists(safe_filename) & file.size(safe_filename) &gt; 1000) {\n        if (verbose) message(sprintf(\"跳过截图: %s (文件已存在)\\n\", link))\n    } else {\n        # 尝试截图\n        tryCatch({\n            webshot2::webshot(url = link, \n                                file = safe_filename, \n                                delay = 5)\n            if (verbose) message(sprintf(\"成功截图: %s\\n\", link))\n        }, error = function(e) {\n            if (verbose) warning(sprintf(\"截图失败: %s, 错误: %s\\n\", link, e$message))\n        })\n    }\n    \n    safe_filename = gsub(\"www/\", \"\", safe_filename)\n    return(safe_filename)\n}\n\n\n\n\n利用 lapply 函数将评论整理为一个列表，然后使用 bind_rows 函数将列表转换为 tibble，方便后续处理。\n\n# 创建一个列表来存储评论信息\ncomments_list = lapply(issue_comments, function(comment) {\n    # 提取评论信息\n    author = comment$user$login\n    content = comment$body\n    content_url = comment$html_url\n    time = comment$created_at\n    id = get_id(comment$body)\n    link = get_link(comment$body)\n    screenshot_path = create_webshot(link, verbose = FALSE)\n\n    # 返回一个 tibble\n    tibble(\n        author = author,\n        content = content,\n        content_url = content_url,\n        time = time,\n        id = id,\n        link = link,\n        screenshot_path = screenshot_path\n    )\n})\n\n# 将列表转换为 tibble\ncomments_df = bind_rows(comments_list)\n\ncomments_df 是一个数据框，包含了评论的作者、内容、评论链接和时间。\n\n\n\n最后，将整理好的评论数据保存为CSV文件，并输出处理结果。对于存在多个链接的评论（），将多个链接用分号分隔，合并为一个评论。\n\n# 保存结果到CSV\nwrite.csv(comments_df, \"github_issue_comments.csv\", row.names = FALSE, fileEncoding = \"UTF-8\")\n\n# 输出结果\ncat(\"处理完成! 共处理了\", nrow(comments_df), \"条评论\\n\")\ncat(\"结果已保存到 github_issue_comments.csv\\n\")\nprint(comments_df)\n\n\n\n\n这个 R 脚本会完成以下任务：\n\n安装并加载必要的R包\n创建一个 webshot 文件夹用于保存网页截图\n从 GitHub API 获取 Issue #1 的内容和所有评论\n将评论整理成一个表格，包含 author、content 和 time 列等\n从评论内容中提取所有链接和学号\n使用 webshot2 包访问链接并生成网页截图\n将截图保存在 webshot 文件夹中\n将截图的文件路径添加到表格中\n将结果保存为CSV文件\n\n\n\n\n利用md5检测当前目录下面的 png 文件，删掉重复的。然后将 png 文件重命名，去掉文件名开始的 \\d+_\\d+_。\n可以使用 R 处理当前目录下的 PNG 文件，完成以下任务：\n\n计算 MD5 哈希值并删除重复的 PNG 文件\n\n重命名 PNG 文件，去掉文件名开头的 \\d+_\\d+_\n\n以下是 R 代码：\n\nlibrary(digest)\n\n# 计算文件 MD5 哈希值\nget_md5 = function(file_path) {\n  digest(file_path, algo = \"md5\", file = TRUE)\n}\n\n# 删除重复的 PNG 文件\nremove_duplicates = function(directory) {\n  files = list.files(directory, pattern = \"\\\\.png$\", full.names = TRUE, ignore.case = TRUE)\n  seen_hashes = list()\n  \n  for (file in files) {\n    file_hash = get_md5(file)\n    \n    if (file_hash %in% names(seen_hashes)) {\n      message(\"删除重复文件: \", file)\n      file.remove(file)\n    } else {\n      seen_hashes[[file_hash]] = file\n    }\n  }\n}\n\n# 重命名 PNG 文件，去掉开头的 \\d+_\\d+_\nrename_files = function(directory) {\n  files = list.files(directory, pattern = \"\\\\.png$\", full.names = TRUE, ignore.case = TRUE)\n  \n  for (file in files) {\n    filename = basename(file)\n    new_name = sub(\"^\\\\d+_\\\\d+_\", \"\", filename)\n    \n    if (new_name != filename) {\n      new_path = file.path(dirname(file), new_name)\n      message(\"重命名: \", file, \" -&gt; \", new_path)\n      file.rename(file, new_path)\n    }\n  }\n}\n\n# 运行\ncurrent_directory = \"webshot\"\nremove_duplicates(current_directory)\nrename_files(current_directory)\n\n\n\n\nget_md5(file_path): 计算 PNG 文件的 MD5 哈希值。\nremove_duplicates(directory): 通过 MD5 识别重复 PNG 文件并删除。\nrename_files(directory): 通过正则表达式 ^\\\\d+_\\\\d+_ 处理文件名，去掉前缀。"
  },
  {
    "objectID": "about.html#准备工作",
    "href": "about.html#准备工作",
    "title": "About",
    "section": "",
    "text": "首先，我们需要安装并加载一些必要的R包，包括 gh、webshot2、dplyr、stringr、tibble、httr 和 jsonlite。如果你尚未安装这些包，可以运行以下代码来安装它们：\n\n# 安装必要的包（如果尚未安装）\nif (!require(\"gh\", quietly = TRUE)) install.packages(\"gh\")\nif (!require(\"webshot2\", quietly = TRUE)) install.packages(\"webshot2\")\nif (!require(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\nif (!require(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\nif (!require(\"tibble\", quietly = TRUE)) install.packages(\"tibble\")\nif (!require(\"httr\", quietly = TRUE)) install.packages(\"httr\")\nif (!require(\"jsonlite\", quietly = TRUE)) install.packages(\"jsonlite\")\n\n# 加载包\nlibrary(gh)\nlibrary(webshot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(httr)\nlibrary(jsonlite)\n\n# 创建webshot文件夹（如果不存在）\nif (!dir.exists(\"www/webshot\")) {\n  dir.create(\"www/webshot\", recursive = TRUE)\n}\n\n这些包的作用如下：\n\ngh：用于访问 GitHub API\nwebshot2：用于生成网页截图\ndplyr：用于数据处理\nstringr：用于字符串处理\ntibble：用于创建数据框\nhttr：用于发起 HTTP 请求\njsonlite：用于处理 JSON 数据"
  },
  {
    "objectID": "about.html#获取评论",
    "href": "about.html#获取评论",
    "title": "About",
    "section": "",
    "text": "接下来，我们将使用 GitHub API 获取指定仓库的 Issue #1 的所有评论。\n首先，将以下代码中的 repo_owner、repo_name 和 issue_number 替换为要爬取的仓库和 Issue 编号：\n\n# 指定仓库和Issue编号\n# 请替换为你需要爬取的仓库\nrepo_owner &lt;- \"D2RS-2025spring\"\nrepo_name &lt;- \"myHomePages\"\nissue_number &lt;- 1\n\n# 使用GitHub API获取issue评论\nissue_comments &lt;- gh::gh(\n  \"GET /repos/{owner}/{repo}/issues/{issue_number}/comments\",\n  owner = repo_owner,\n  repo = repo_name,\n  issue_number = issue_number,\n  .limit = Inf\n)\n\n在这里使用 .limit = Inf 来获取所有评论，如果评论数量较多，可能需要等待一段时间。"
  },
  {
    "objectID": "about.html#打印原始-issue-内容",
    "href": "about.html#打印原始-issue-内容",
    "title": "About",
    "section": "",
    "text": "在获取 ISSUE comments 的方法中，并不会包含第一条的评论（即 Issue 的标题和内容），所以我们需要单独获取 Issue 的标题和内容，打印原始 Issue 的标题和内容，以便了解 Issue 的背景信息。\n\n# 获取原始issue内容\nissue_data &lt;- gh::gh(\n  \"GET /repos/{owner}/{repo}/issues/{issue_number}\",\n  owner = repo_owner,\n  repo = repo_name,\n  issue_number = issue_number\n)\n\ncli::cat_rule(\"原始Issue内容\")\ncat(issue_data$title, \"\\n\")\ncat(issue_data$body, \"\\n\")"
  },
  {
    "objectID": "about.html#创建辅助函数",
    "href": "about.html#创建辅助函数",
    "title": "About",
    "section": "",
    "text": "get() 函数用于从评论内容中提取匹配的内容。如果没有匹配，会打印警告；如果有多个匹配，会保留第一个；如果只有一个匹配，直接返回。\nget_id() 这里使用的正则表达式为 (?&lt;!\\\\d)\\\\d{13}(?!\\\\d)，可以匹配 13 位数字，前后不是数字的情况。其中 (?&lt;!\\\\d) 表示前面不是数字，\\\\d{13} 表示匹配 13 位数字，(?!\\\\d) 表示后面不是数字。\nget_link() 这里使用的正则表达式为 https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+，可以匹配直接 URL。其中 https?:// 表示匹配 http:// 或 https://，[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+ 表示匹配除空格、括号、尖括号、引号和方括号之外的字符。\ncreate_webshot() 用于生成网页截图，如果文件已经存在且大小大于 1 kb，则跳过。如果文件不存在或大小小于 1 kb，则尝试生成网页截图。如果生成成功，则打印成功信息；如果生成失败，则打印失败信息。\n\nget = function(content, pattern){\n    matches &lt;- str_extract_all(content, pattern)[[1]]\n    matches &lt;- unique(matches)\n    match &lt;- NULL\n    if (length(matches) &lt; 1) {\n        # 如果没有匹配，打印警告\n        warning(glue::glue(\"{content} 中未检测到: {pattern}\"))\n    } else if (length(matches) &gt; 1) {\n        # 如果有多个匹配，只取第一个\n        match &lt;- matches[1]\n        message(glue::glue(\"{content} 中检测到多个匹配: `{pattern}`。保留第一个。\"))\n    } else {\n        # 如果只有一个匹配，直接取出\n        match = matches[1]\n    }\n    return(match)\n}\n\nget_id = function(content){\n    get(content, pattern = \"(?&lt;!\\\\d)\\\\d{13}(?!\\\\d)\")\n}\n\nget_link = function(content){\n    get(content, pattern = \"https?://[^\\\\s()&lt;&gt;\\\"\\\\[\\\\]]+\")\n}\n\ncreate_webshot = function(link, verbose = TRUE) {\n    # valid link\n    if (is.null(link) || !str_detect(link, \"^https?://\")) {\n        warning(glue::glue(\"Invalid link: {link}\"))\n        return(NULL)\n    }\n\n    # 创建一个安全的文件名\n    safe_filename &lt;- paste0(\"www/webshot/\", \n                            str_replace_all(gsub(\"https?://\", \"\", link), \"[^a-zA-Z0-9]\", \"_\"), \n                            \".png\")\n    \n    # 如果文件已经存在且大小大于 1 kb，跳过\n    if (file.exists(safe_filename) & file.size(safe_filename) &gt; 1000) {\n        if (verbose) message(sprintf(\"跳过截图: %s (文件已存在)\\n\", link))\n    } else {\n        # 尝试截图\n        tryCatch({\n            webshot2::webshot(url = link, \n                                file = safe_filename, \n                                delay = 5)\n            if (verbose) message(sprintf(\"成功截图: %s\\n\", link))\n        }, error = function(e) {\n            if (verbose) warning(sprintf(\"截图失败: %s, 错误: %s\\n\", link, e$message))\n        })\n    }\n    \n    safe_filename = gsub(\"www/\", \"\", safe_filename)\n    return(safe_filename)\n}"
  },
  {
    "objectID": "about.html#整理评论",
    "href": "about.html#整理评论",
    "title": "About",
    "section": "",
    "text": "利用 lapply 函数将评论整理为一个列表，然后使用 bind_rows 函数将列表转换为 tibble，方便后续处理。\n\n# 创建一个列表来存储评论信息\ncomments_list = lapply(issue_comments, function(comment) {\n    # 提取评论信息\n    author = comment$user$login\n    content = comment$body\n    content_url = comment$html_url\n    time = comment$created_at\n    id = get_id(comment$body)\n    link = get_link(comment$body)\n    screenshot_path = create_webshot(link, verbose = FALSE)\n\n    # 返回一个 tibble\n    tibble(\n        author = author,\n        content = content,\n        content_url = content_url,\n        time = time,\n        id = id,\n        link = link,\n        screenshot_path = screenshot_path\n    )\n})\n\n# 将列表转换为 tibble\ncomments_df = bind_rows(comments_list)\n\ncomments_df 是一个数据框，包含了评论的作者、内容、评论链接和时间。"
  },
  {
    "objectID": "about.html#保存结果",
    "href": "about.html#保存结果",
    "title": "About",
    "section": "",
    "text": "最后，将整理好的评论数据保存为CSV文件，并输出处理结果。对于存在多个链接的评论（），将多个链接用分号分隔，合并为一个评论。\n\n# 保存结果到CSV\nwrite.csv(comments_df, \"github_issue_comments.csv\", row.names = FALSE, fileEncoding = \"UTF-8\")\n\n# 输出结果\ncat(\"处理完成! 共处理了\", nrow(comments_df), \"条评论\\n\")\ncat(\"结果已保存到 github_issue_comments.csv\\n\")\nprint(comments_df)"
  },
  {
    "objectID": "about.html#小结",
    "href": "about.html#小结",
    "title": "About",
    "section": "",
    "text": "这个 R 脚本会完成以下任务：\n\n安装并加载必要的R包\n创建一个 webshot 文件夹用于保存网页截图\n从 GitHub API 获取 Issue #1 的内容和所有评论\n将评论整理成一个表格，包含 author、content 和 time 列等\n从评论内容中提取所有链接和学号\n使用 webshot2 包访问链接并生成网页截图\n将截图保存在 webshot 文件夹中\n将截图的文件路径添加到表格中\n将结果保存为CSV文件"
  },
  {
    "objectID": "about.html#附加",
    "href": "about.html#附加",
    "title": "About",
    "section": "",
    "text": "利用md5检测当前目录下面的 png 文件，删掉重复的。然后将 png 文件重命名，去掉文件名开始的 \\d+_\\d+_。\n可以使用 R 处理当前目录下的 PNG 文件，完成以下任务：\n\n计算 MD5 哈希值并删除重复的 PNG 文件\n\n重命名 PNG 文件，去掉文件名开头的 \\d+_\\d+_\n\n以下是 R 代码：\n\nlibrary(digest)\n\n# 计算文件 MD5 哈希值\nget_md5 = function(file_path) {\n  digest(file_path, algo = \"md5\", file = TRUE)\n}\n\n# 删除重复的 PNG 文件\nremove_duplicates = function(directory) {\n  files = list.files(directory, pattern = \"\\\\.png$\", full.names = TRUE, ignore.case = TRUE)\n  seen_hashes = list()\n  \n  for (file in files) {\n    file_hash = get_md5(file)\n    \n    if (file_hash %in% names(seen_hashes)) {\n      message(\"删除重复文件: \", file)\n      file.remove(file)\n    } else {\n      seen_hashes[[file_hash]] = file\n    }\n  }\n}\n\n# 重命名 PNG 文件，去掉开头的 \\d+_\\d+_\nrename_files = function(directory) {\n  files = list.files(directory, pattern = \"\\\\.png$\", full.names = TRUE, ignore.case = TRUE)\n  \n  for (file in files) {\n    filename = basename(file)\n    new_name = sub(\"^\\\\d+_\\\\d+_\", \"\", filename)\n    \n    if (new_name != filename) {\n      new_path = file.path(dirname(file), new_name)\n      message(\"重命名: \", file, \" -&gt; \", new_path)\n      file.rename(file, new_path)\n    }\n  }\n}\n\n# 运行\ncurrent_directory = \"webshot\"\nremove_duplicates(current_directory)\nrename_files(current_directory)\n\n\n\n\nget_md5(file_path): 计算 PNG 文件的 MD5 哈希值。\nremove_duplicates(directory): 通过 MD5 识别重复 PNG 文件并删除。\nrename_files(directory): 通过正则表达式 ^\\\\d+_\\\\d+_ 处理文件名，去掉前缀。"
  }
]